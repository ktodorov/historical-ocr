{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from enums.language import Language\n",
    "from enums.configuration import Configuration\n",
    "from enums.ocr_output_type import OCROutputType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "def get_folder_paths(language: Language):\n",
    "    newseye_path = os.path.join('..', 'data', 'newseye')\n",
    "    icdar_2017_path = os.path.join(newseye_path, '2017', 'full')\n",
    "    icdar_2019_path = os.path.join(newseye_path, '2019', 'full')\n",
    "\n",
    "    result = None\n",
    "    if language == Language.English:\n",
    "        result = [\n",
    "            os.path.join(icdar_2017_path, 'eng_monograph'),\n",
    "            os.path.join(icdar_2017_path, 'eng_periodical'),\n",
    "            os.path.join(icdar_2019_path, 'EN')\n",
    "        ]\n",
    "    elif language == Language.Dutch:\n",
    "        result = [\n",
    "            os.path.join(icdar_2019_path, 'NL', 'NL1')\n",
    "        ]\n",
    "    elif language == Language.French:\n",
    "        result = [\n",
    "            os.path.join(icdar_2017_path, 'fr_monograph'),\n",
    "            os.path.join(icdar_2017_path, 'fr_periodical'),\n",
    "            os.path.join(icdar_2019_path, 'FR', 'FR1'),\n",
    "            os.path.join(icdar_2019_path, 'FR', 'FR2'),\n",
    "            os.path.join(icdar_2019_path, 'FR', 'FR3')\n",
    "        ]\n",
    "    elif language == Language.German:\n",
    "        result = [\n",
    "            os.path.join(icdar_2019_path, 'DE', 'DE1'),\n",
    "            os.path.join(icdar_2019_path, 'DE', 'DE2'),\n",
    "            os.path.join(icdar_2019_path, 'DE', 'DE3'),\n",
    "            os.path.join(icdar_2019_path, 'DE', 'DE4'),\n",
    "            os.path.join(icdar_2019_path, 'DE', 'DE5'),\n",
    "            os.path.join(icdar_2019_path, 'DE', 'DE6'),\n",
    "            os.path.join(icdar_2019_path, 'DE', 'DE7')\n",
    "        ]\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents(tokenizer, language: Language, ocr_output_type: OCROutputType):\n",
    "    documents = []\n",
    "\n",
    "    folder_paths = get_folder_paths(language)\n",
    "    for folder_path in folder_paths:\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as text_file:\n",
    "                file_lines = text_file.readlines()\n",
    "                gt_line = file_lines[2] if ocr_output_type == OCROutputType.GroundTruth else file_lines[1]\n",
    "                processed_line = gt_line[14:].replace('#', '').replace('@', '')\n",
    "\n",
    "                text_nonum = re.sub(r'\\d+', '', processed_line)\n",
    "                text_nopunct = \"\".join([char.lower() for char in text_nonum if char not in string.punctuation])\n",
    "                text_no_doublespace = re.sub('\\s+', ' ', text_nopunct).strip()\n",
    "                result = tokenizer.tokenize(text_no_doublespace)\n",
    "                documents.append(result)\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in [Language.Dutch, Language.English, Language.French, Language.German]:\n",
    "    for ocr_output_type in OCROutputType:\n",
    "        glove_filepath = os.path.join('results', 'glove')\n",
    "        result_filepath = os.path.join(glove_filepath, f'{language.value}_{ocr_output_type.value}_corpus.txt')\n",
    "        if os.path.exists(result_filepath):\n",
    "            continue\n",
    "\n",
    "        documents = read_documents(tokenizer, language, ocr_output_type)\n",
    "\n",
    "        if not os.path.exists(glove_filepath):\n",
    "            os.mkdir(glove_filepath)\n",
    "\n",
    "        with open(result_filepath, 'w', encoding='utf-8') as result_file:\n",
    "            for document in documents:\n",
    "                document_str = ' '.join(document)\n",
    "                if len(document_str.strip()) == 0: continue\n",
    "\n",
    "                result_file.write(document_str)\n",
    "                result_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "\n",
    "def save_python_obj(obj: object, path: str, name: str) -> bool:\n",
    "    try:\n",
    "        filepath = os.path.join(path, f'{name}.pickle')\n",
    "        with open(filepath, 'wb') as handle:\n",
    "            pickle.dump(obj, handle, protocol=-1)\n",
    "\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def load_python_obj(path: str, name: str, extension_included: bool = False) -> object:\n",
    "    obj = None\n",
    "    try:\n",
    "        extension = '' if extension_included else '.pickle'\n",
    "        filepath = os.path.join(path, f'{name}{extension}')\n",
    "        with (open(filepath, \"rb\")) as openfile:\n",
    "            obj = pickle.load(openfile)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_filepath = os.path.join('results', 'glove', 'vectors')\n",
    "\n",
    "vectors_by_words = load_python_obj(vectors_filepath, 'vectors-by-words')\n",
    "if vectors_by_words is None:\n",
    "    vectors_by_words = {}\n",
    "\n",
    "    for language in [Language.Dutch, Language.English, Language.French, Language.German]:\n",
    "        vectors_by_words[language] = {}\n",
    "        for ocr_output_type in OCROutputType:\n",
    "            vectors_by_words[language][ocr_output_type] = {}\n",
    "            filepath = os.path.join(vectors_filepath, f'{language.value}_{ocr_output_type.value}_vectors.txt')\n",
    "            print(filepath)\n",
    "            with open(filepath, 'rb') as vectors_file:\n",
    "                for i, line in enumerate(vectors_file.readlines()):\n",
    "                    split_line = line.split()\n",
    "                    word = split_line[0]\n",
    "\n",
    "                    # if i == 1675:\n",
    "                    #     print(word)\n",
    "                    #     print(str(word) == 'tte')\n",
    "\n",
    "                    numbers = np.array([float(x) for x in split_line[1:]])\n",
    "\n",
    "                    try:\n",
    "                        vectors_by_words[language][ocr_output_type][word.decode()] = numbers\n",
    "                    except:\n",
    "                        print(f'Failed for word {word}')\n",
    "\n",
    "                print(len(vectors_by_words[language][ocr_output_type].keys()))\n",
    "\n",
    "    save_python_obj(vectors_by_words, vectors_filepath, 'vectors-by-words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\.cache\\ocr-evaluation\\dutch\\ppmi\n",
      "..\\.cache\\ocr-evaluation\\french\\ppmi\n"
     ]
    }
   ],
   "source": [
    "unique_tokens = {}\n",
    "\n",
    "for language in [Language.Dutch, Language.English, Language.French, Language.German]:\n",
    "    unique_tokens[language] = None\n",
    "\n",
    "    for config in [Configuration.SkipGram, Configuration.CBOW, Configuration.PPMI]:\n",
    "        cache_path = os.path.join('..', '.cache', 'ocr-evaluation', language.value, config.value)\n",
    "        raw_vocab_obj = load_python_obj(cache_path, f'vocab-icdar-2017-icdar-2019-{OCROutputType.Raw.value}')\n",
    "        gt_vocab_obj = load_python_obj(cache_path, f'vocab-icdar-2017-icdar-2019-{OCROutputType.GroundTruth.value}')\n",
    "        if raw_vocab_obj is None or gt_vocab_obj is None:\n",
    "            print(cache_path)\n",
    "            continue\n",
    "\n",
    "        # extract the tokens from the vocabularies\n",
    "        raw_tokens = list(raw_vocab_obj[0].keys())[4:]\n",
    "        gt_tokens = list(gt_vocab_obj[0].keys())[4:]\n",
    "        intersected_tokens = list(set(raw_tokens) & set(gt_tokens))\n",
    "\n",
    "        if unique_tokens[language] is None:\n",
    "            unique_tokens[language] = intersected_tokens\n",
    "        else:\n",
    "            unique_tokens[language] = list(set(intersected_tokens) & set(unique_tokens[language]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = { language: {\n",
    "    word: idx for (idx, word) in enumerate(unique_tokens[language])\n",
    "}\n",
    "    for language in unique_tokens.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dutch\n",
      "Processing english\n",
      "Processing french\n",
      "53272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tokens for 'french': 100%|██████████| 18391/18391 [39:22<00:00,  7.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing german\n",
      "72732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tokens for 'german': 100%|██████████| 8036/8036 [07:28<00:00, 17.91it/s]\n"
     ]
    }
   ],
   "source": [
    "overlaps = {}\n",
    "percentages = list(range(1, 101, 1))  # 1..20\n",
    "\n",
    "for language in [Language.Dutch, Language.English, Language.French, Language.German]:\n",
    "    print(f'Processing {language}')\n",
    "    words_amounts = [\n",
    "        int(len(unique_tokens[language]) * (float(percentage)/ 100))\n",
    "        for percentage in percentages]\n",
    "\n",
    "    max_n = max(words_amounts)\n",
    "\n",
    "    cached_name = f'overlaps_{language.value}_glove'\n",
    "    cached_value = load_python_obj('results', cached_name)\n",
    "    if cached_value is not None:\n",
    "        overlaps[language] = cached_value\n",
    "        continue\n",
    "\n",
    "    overlaps[language] = { percentage : { token: [] for token in unique_tokens[language] } for percentage in percentages }\n",
    "\n",
    "    print(len(vectors_by_words[language][OCROutputType.Raw].keys()))\n",
    "    raw_vectors = np.array([vectors_by_words[language][OCROutputType.Raw][word] for word in unique_tokens[language]])\n",
    "    raw_similarity = 1 - cdist(raw_vectors, raw_vectors, metric='cosine')\n",
    "    gt_vectors = np.array([vectors_by_words[language][OCROutputType.GroundTruth][word] for word in unique_tokens[language]])\n",
    "    gt_similarity = 1 - cdist(gt_vectors, gt_vectors, metric='cosine')\n",
    "\n",
    "    for token in tqdm(unique_tokens[language], desc=f'Processing tokens for \\'{language.value}\\'', total=len(unique_tokens[language])):\n",
    "        raw_indices = np.argsort(raw_similarity[_[language][token]])[::-1][:max_n]\n",
    "        gt_indices = np.argsort(gt_similarity[_[language][token]])[::-1][:max_n]\n",
    "\n",
    "        for n, percentage in zip(words_amounts, percentages):\n",
    "            current_gt = gt_indices[:n]\n",
    "            current_raw = raw_indices[:n]\n",
    "\n",
    "            current_overlaps = len(set(current_gt) & set(current_raw))\n",
    "            overlaps[language][percentage][token].append(current_overlaps)\n",
    "\n",
    "    save_python_obj(overlaps[language], 'results', cached_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8b75f3199d40dc5b32c50d7e73d50b0653ef4e42fc29a92d9ddb9ff9d4b03964"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ocr': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
