{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\r\n",
    "from gensim.models import Word2Vec\r\n",
    "\r\n",
    "import os\r\n",
    "import re\r\n",
    "import string\r\n",
    "import numpy as np\r\n",
    "from tqdm import tqdm\r\n",
    "import _pickle as pickle\r\n",
    "\r\n",
    "from nltk.tokenize import RegexpTokenizer\r\n",
    "\r\n",
    "import sys\r\n",
    "sys.path.insert(0, '..')\r\n",
    "\r\n",
    "from enums.language import Language\r\n",
    "from enums.configuration import Configuration\r\n",
    "from enums.ocr_output_type import OCROutputType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\r\n",
    "\r\n",
    "def get_folder_paths(language: Language):\r\n",
    "    newseye_path = os.path.join('..', 'data', 'newseye')\r\n",
    "\r\n",
    "    result = None\r\n",
    "    if language == Language.English:\r\n",
    "        icdar_2017_1_path = os.path.join(newseye_path, '2017', 'full', 'eng_monograph')\r\n",
    "        icdar_2017_2_path = os.path.join(newseye_path, '2017', 'full', 'eng_periodical')\r\n",
    "        icdar_2019_path = os.path.join(newseye_path, '2019', 'full', 'EN')\r\n",
    "        result = [icdar_2017_1_path, icdar_2017_2_path, icdar_2019_path]\r\n",
    "    elif language == Language.Dutch:\r\n",
    "        icdar_2019_path = os.path.join(newseye_path, '2019', 'full', 'NL', 'NL1')\r\n",
    "        result = [icdar_2019_path]\r\n",
    "\r\n",
    "    return result\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents(tokenizer, language: Language, ocr_output_type: OCROutputType):\r\n",
    "    documents = []\r\n",
    "\r\n",
    "    folder_paths = get_folder_paths(language)\r\n",
    "    for folder_path in folder_paths:\r\n",
    "        for filename in os.listdir(folder_path):\r\n",
    "            file_path = os.path.join(folder_path, filename)\r\n",
    "            with open(file_path, 'r', encoding='utf-8') as text_file:\r\n",
    "                file_lines = text_file.readlines()\r\n",
    "                gt_line = file_lines[2] if ocr_output_type == OCROutputType.GroundTruth else file_lines[1]\r\n",
    "                processed_line = gt_line[14:].replace('#', '').replace('@', '')\r\n",
    "\r\n",
    "                text_nonum = re.sub(r'\\d+', '', processed_line)\r\n",
    "                text_nopunct = \"\".join([char.lower() for char in text_nonum if char not in string.punctuation])\r\n",
    "                text_no_doublespace = re.sub('\\s+', ' ', text_nopunct).strip()\r\n",
    "                result = tokenizer.tokenize(text_no_doublespace)\r\n",
    "                documents.append(result)\r\n",
    "\r\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_path(\r\n",
    "    language: Language,\r\n",
    "    configuration: Configuration,\r\n",
    "    randomly_initialized: bool,\r\n",
    "    ocr_output_type: OCROutputType,\r\n",
    "    learning_rate: float):\r\n",
    "    rnd_suffix = 'random' if randomly_initialized else 'pretr'\r\n",
    "\r\n",
    "    model_name = f'gensim_{language.value}_{configuration.value}_{rnd_suffix}_{ocr_output_type.value}_lr{learning_rate}.model'\r\n",
    "\r\n",
    "    results_folder = 'results'\r\n",
    "    if not os.path.exists(results_folder):\r\n",
    "        os.mkdir(results_folder)\r\n",
    "\r\n",
    "    result = os.path.join(results_folder, model_name)\r\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\r\n",
    "    if not os.path.exists(model_path):\r\n",
    "        return None\r\n",
    "\r\n",
    "    model = Word2Vec.load(model_path)\r\n",
    "    return model\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_model_info(language: Language):\r\n",
    "    if language == Language.English:\r\n",
    "        return 'GoogleNews-vectors-negative300.bin', True\r\n",
    "    elif language == Language.Dutch:\r\n",
    "        return 'combined-320.txt', False\r\n",
    "    elif language == Language.French:\r\n",
    "        return 'frwiki_20180420_300d.txt', False\r\n",
    "    elif language == Language.German:\r\n",
    "        return 'dewiki_20180420_300d.txt', False\r\n",
    "\r\n",
    "    error_message = 'Unsupported word2vec language'\r\n",
    "    raise Exception(error_message)\r\n",
    "\r\n",
    "def get_pretrained_matrix(language: Language):\r\n",
    "    data_path = os.path.join('..', 'data', 'ocr-evaluation', 'word2vec', language.value)\r\n",
    "    word2vec_model_name, word2vec_binary = get_word2vec_model_info(language)\r\n",
    "    word2vec_model_path = os.path.join(data_path, word2vec_model_name)\r\n",
    "    word2vec_model  = gensim.models.KeyedVectors.load_word2vec_format(word2vec_model_path, binary=word2vec_binary)\r\n",
    "    return word2vec_model, word2vec_model_path, word2vec_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\r\n",
    "\r\n",
    "def create_model(\r\n",
    "    corpus,\r\n",
    "    model_path: str,\r\n",
    "    configuration: Configuration,\r\n",
    "    randomly_initialized: bool,\r\n",
    "    language: Language,\r\n",
    "    learning_rate: float):\r\n",
    "    sg = 1 if configuration == Configuration.SkipGram else 0\r\n",
    "    vector_size = 320 if language == Language.Dutch else 300\r\n",
    "\r\n",
    "    # initialize the model\r\n",
    "    model = Word2Vec(vector_size=vector_size, window=5, min_count=5, workers=2, sg=sg, alpha=learning_rate)\r\n",
    "\r\n",
    "    # build the vocabulary\r\n",
    "    model.build_vocab(corpus, progress_per=1000)\r\n",
    "\r\n",
    "    if not randomly_initialized:\r\n",
    "        word2vec_weights, word2vec_model_path, word2vec_binary = get_pretrained_matrix(language)\r\n",
    "        model.build_vocab(list(word2vec_weights.key_to_index.keys()), update=True)\r\n",
    "        model.wv.vectors_lockf = np.ones((len(model.wv.key_to_index), 1)) # fix for word2vec issue\r\n",
    "        model.wv.intersect_word2vec_format(word2vec_model_path, binary=word2vec_binary, lockf=1.0)\r\n",
    "\r\n",
    "    # train the model\r\n",
    "    model.train(corpus, total_examples=model.corpus_count, epochs=300, report_delay=1)\r\n",
    "\r\n",
    "    # save the model\r\n",
    "    model.save(model_path)\r\n",
    "\r\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: ['dutch', cbow, lr: 0.001, True, ground-truth]\n",
      "Training: ['dutch', cbow, lr: 0.001, True, raw]\n",
      "Training: ['dutch', cbow, lr: 0.0001, True, ground-truth]\n",
      "Training: ['dutch', cbow, lr: 0.0001, True, raw]\n",
      "Training: ['dutch', skip-gram, lr: 0.001, True, ground-truth]\n",
      "Training: ['dutch', skip-gram, lr: 0.001, True, raw]\n",
      "Training: ['dutch', skip-gram, lr: 0.0001, True, ground-truth]\n",
      "Training: ['dutch', skip-gram, lr: 0.0001, True, raw]\n",
      "Training: ['english', cbow, lr: 0.001, True, ground-truth]\n",
      "Training: ['english', cbow, lr: 0.001, True, raw]\n",
      "Training: ['english', cbow, lr: 0.0001, True, ground-truth]\n",
      "Training: ['english', cbow, lr: 0.0001, True, raw]\n",
      "Training: ['english', skip-gram, lr: 0.001, True, ground-truth]\n",
      "Training: ['english', skip-gram, lr: 0.001, True, raw]\n",
      "Training: ['english', skip-gram, lr: 0.0001, True, ground-truth]\n",
      "Training: ['english', skip-gram, lr: 0.0001, True, raw]\n"
     ]
    }
   ],
   "source": [
    "unique_tokens = {}\r\n",
    "models = {}\r\n",
    "\r\n",
    "for language in [Language.Dutch, Language.English]:\r\n",
    "    models[language] = {}\r\n",
    "    unique_tokens[language] = None\r\n",
    "    for configuration in [Configuration.CBOW, Configuration.SkipGram]:\r\n",
    "        models[language][configuration] = {}\r\n",
    "        for learning_rate in [0.001, 0.0001]:\r\n",
    "            models[language][configuration][learning_rate] = {}\r\n",
    "            for randomly_initialized in [True]:\r\n",
    "                models[language][configuration][learning_rate][randomly_initialized] = {}\r\n",
    "                for ocr_output_type in [OCROutputType.GroundTruth, OCROutputType.Raw]:\r\n",
    "                    print(f'Training: [\\'{language.value}\\', {configuration.value}, lr: {learning_rate}, {randomly_initialized}, {ocr_output_type.value}]')\r\n",
    "                    documents = read_documents(tokenizer, language, ocr_output_type)\r\n",
    "                    model_path = get_model_path(language, configuration, randomly_initialized, ocr_output_type, learning_rate)\r\n",
    "                    model = load_model(model_path)\r\n",
    "                    if model is None:\r\n",
    "                        print('Model is not loaded. Creating and training now...')\r\n",
    "                        model = create_model(documents, model_path, configuration, randomly_initialized, language, learning_rate)\r\n",
    "\r\n",
    "                    models[language][configuration][learning_rate][randomly_initialized][ocr_output_type] = model\r\n",
    "                    tokens = list(model.wv.key_to_index.keys())\r\n",
    "                    if unique_tokens[language] is None:\r\n",
    "                        unique_tokens[language] = tokens\r\n",
    "                    else:\r\n",
    "                        unique_tokens[language] = list(set(tokens) & set(unique_tokens[language]))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_words = {\r\n",
    "#     Language.English: ['man', 'new', 'time', 'day', 'good', 'old', 'little', 'one', 'two', 'three'],\r\n",
    "#     Language.Dutch: ['man', 'jaar', 'tijd', 'dag', 'huis', 'dier', 'werk', 'naam', 'groot', 'kleine', 'twee', 'drie', 'vier', 'vijf']\r\n",
    "# }\r\n",
    "\r\n",
    "# for word in target_words[language]:\r\n",
    "#     print(f'-- \\'{word}\\':')\r\n",
    "#     print(model.wv.most_similar(positive=[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_python_obj(obj: object, path: str, name: str) -> bool:\r\n",
    "    try:\r\n",
    "        filepath = os.path.join(path, f'{name}.pickle')\r\n",
    "        with open(filepath, 'wb') as handle:\r\n",
    "            pickle.dump(obj, handle, protocol=-1)\r\n",
    "\r\n",
    "        return True\r\n",
    "    except Exception:\r\n",
    "        return False\r\n",
    "\r\n",
    "def load_python_obj(path: str, name: str, extension_included: bool = False) -> object:\r\n",
    "    obj = None\r\n",
    "    try:\r\n",
    "        extension = '' if extension_included else '.pickle'\r\n",
    "        filepath = os.path.join(path, f'{name}{extension}')\r\n",
    "        with (open(filepath, \"rb\")) as openfile:\r\n",
    "            obj = pickle.load(openfile)\r\n",
    "\r\n",
    "    except FileNotFoundError:\r\n",
    "        return None\r\n",
    "\r\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tokens for 'english, cbow, lr: 0.001, random': 100%|██████████| 13338/13338 [57:03<00:00,  3.90it/s]\n",
      "Processing tokens for 'english, cbow, lr: 0.0001, random': 100%|██████████| 13338/13338 [1:00:40<00:00,  3.66it/s]\n",
      "Processing tokens for 'english, skip-gram, lr: 0.001, random':   1%|          | 128/13338 [00:36<1:02:26,  3.53it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-4f77966a3222>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m                         \u001b[0mcurrent_gt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgt_most_similar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                         \u001b[0mcurrent_raw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw_most_similar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                         \u001b[0mcurrent_overlaps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_gt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_raw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m                         \u001b[0moverlaps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrandomly_initialized\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpercentage\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_overlaps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "overlaps = {}\r\n",
    "percentages = list(range(1, 101, 1))  # 1..20\r\n",
    "\r\n",
    "for language in [Language.Dutch, Language.English]:\r\n",
    "    overlaps[language] = {}\r\n",
    "    words_amounts = [ \r\n",
    "        int(len(unique_tokens[language]) * (float(percentage)/ 100))\r\n",
    "        for percentage in percentages]\r\n",
    "\r\n",
    "    max_n = max(words_amounts)\r\n",
    "\r\n",
    "    for configuration in [Configuration.CBOW, Configuration.SkipGram]:\r\n",
    "        overlaps[language][configuration] = {}\r\n",
    "        for learning_rate in [0.001, 0.0001]:\r\n",
    "            overlaps[language][configuration][learning_rate] = {}\r\n",
    "            for randomly_initialized in [True]:\r\n",
    "                rnd_suffix = 'random' if randomly_initialized else 'pretr'\r\n",
    "                cached_name = f'overlaps_{language.value}_{configuration.value}_lr{learning_rate}_{rnd_suffix}'\r\n",
    "                cached_value = load_python_obj('results', cached_name)\r\n",
    "                if cached_value is not None:\r\n",
    "                    overlaps[language][configuration][learning_rate][randomly_initialized] = cached_value\r\n",
    "                    continue\r\n",
    "\r\n",
    "                overlaps[language][configuration][learning_rate][randomly_initialized] = { percentage : { token: [] for token in unique_tokens[language] } for percentage in percentages }\r\n",
    "                for token in tqdm(unique_tokens[language], desc=f'Processing tokens for \\'{language.value}, {configuration.value}, lr: {learning_rate}, {rnd_suffix}\\'', total=len(unique_tokens[language])):\r\n",
    "                    gt_most_similar = models[language][configuration][learning_rate][randomly_initialized][OCROutputType.GroundTruth].wv.most_similar(token, topn=max_n)\r\n",
    "                    gt_most_similar = [x[0] for x in gt_most_similar]\r\n",
    "                    raw_most_similar = models[language][configuration][learning_rate][randomly_initialized][OCROutputType.Raw].wv.most_similar(token, topn=max_n)\r\n",
    "                    raw_most_similar = [x[0] for x in raw_most_similar]\r\n",
    "\r\n",
    "                    for n, percentage in zip(words_amounts, percentages):\r\n",
    "                        current_gt = gt_most_similar[:n]\r\n",
    "                        current_raw = raw_most_similar[:n]\r\n",
    "                        current_overlaps = len(set(current_gt) & set(current_raw))\r\n",
    "                        overlaps[language][configuration][learning_rate][randomly_initialized][percentage][token].append(current_overlaps)\r\n",
    "\r\n",
    "                save_python_obj(overlaps[language][configuration][learning_rate][randomly_initialized], 'results', cached_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc29f658ddb1b0f0a648f4c47acf5938bc6d1ad3f68ae93354e191176a755a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('ocr-uva': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}