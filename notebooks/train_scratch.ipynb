{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import _pickle as pickle\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from enums.language import Language\n",
    "from enums.configuration import Configuration\n",
    "from enums.ocr_output_type import OCROutputType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "def get_folder_paths(language: Language):\n",
    "    newseye_path = os.path.join('..', 'data', 'newseye')\n",
    "    icdar_2017_path = os.path.join(newseye_path, '2017', 'full')\n",
    "    icdar_2019_path = os.path.join(newseye_path, '2019', 'full')\n",
    "\n",
    "    result = None\n",
    "    if language == Language.English:\n",
    "        result = [\n",
    "            os.path.join(icdar_2017_path, 'eng_monograph'),\n",
    "            os.path.join(icdar_2017_path, 'eng_periodical'),\n",
    "            os.path.join(icdar_2019_path, 'EN')\n",
    "        ]\n",
    "    elif language == Language.Dutch:\n",
    "        result = [\n",
    "            os.path.join(icdar_2019_path, 'NL', 'NL1')\n",
    "        ]\n",
    "    elif language == Language.French:\n",
    "        result = [\n",
    "            os.path.join(icdar_2017_path, 'fr_monograph'),\n",
    "            os.path.join(icdar_2017_path, 'fr_periodical'),\n",
    "            os.path.join(icdar_2019_path, 'FR', 'FR1'),\n",
    "            os.path.join(icdar_2019_path, 'FR', 'FR2'),\n",
    "            os.path.join(icdar_2019_path, 'FR', 'FR3')\n",
    "        ]\n",
    "    elif language == Language.German:\n",
    "        result = [\n",
    "            os.path.join(icdar_2019_path, 'DE', 'DE1'),\n",
    "            os.path.join(icdar_2019_path, 'DE', 'DE2'),\n",
    "            os.path.join(icdar_2019_path, 'DE', 'DE3'),\n",
    "            os.path.join(icdar_2019_path, 'DE', 'DE4'),\n",
    "            os.path.join(icdar_2019_path, 'DE', 'DE5'),\n",
    "            os.path.join(icdar_2019_path, 'DE', 'DE6'),\n",
    "            os.path.join(icdar_2019_path, 'DE', 'DE7')\n",
    "        ]\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents(tokenizer, language: Language, ocr_output_type: OCROutputType):\n",
    "    documents = []\n",
    "\n",
    "    folder_paths = get_folder_paths(language)\n",
    "    for folder_path in folder_paths:\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as text_file:\n",
    "                file_lines = text_file.readlines()\n",
    "                gt_line = file_lines[2] if ocr_output_type == OCROutputType.GroundTruth else file_lines[1]\n",
    "                processed_line = gt_line[14:].replace('#', '').replace('@', '')\n",
    "\n",
    "                text_nonum = re.sub(r'\\d+', '', processed_line)\n",
    "                text_nopunct = \"\".join([char.lower() for char in text_nonum if char not in string.punctuation])\n",
    "                text_no_doublespace = re.sub('\\s+', ' ', text_nopunct).strip()\n",
    "                result = tokenizer.tokenize(text_no_doublespace)\n",
    "                documents.append(result)\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_path(\n",
    "    language: Language,\n",
    "    configuration: Configuration,\n",
    "    randomly_initialized: bool,\n",
    "    ocr_output_type: OCROutputType,\n",
    "    learning_rate: float):\n",
    "    rnd_suffix = 'random' if randomly_initialized else 'pretr'\n",
    "\n",
    "    model_name = f'gensim_{language.value}_{configuration.value}_{rnd_suffix}_{ocr_output_type.value}_lr{learning_rate}.model'\n",
    "\n",
    "    results_folder = 'results'\n",
    "    if not os.path.exists(results_folder):\n",
    "        os.mkdir(results_folder)\n",
    "\n",
    "    result = os.path.join(results_folder, model_name)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    if not os.path.exists(model_path):\n",
    "        return None\n",
    "\n",
    "    model = Word2Vec.load(model_path)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_model_info(language: Language):\n",
    "    if language == Language.English:\n",
    "        return 'GoogleNews-vectors-negative300.bin', True\n",
    "    elif language == Language.Dutch:\n",
    "        return 'combined-320.txt', False\n",
    "    elif language == Language.French:\n",
    "        return 'frwiki_20180420_300d.txt', False\n",
    "    elif language == Language.German:\n",
    "        return 'dewiki_20180420_300d.txt', False\n",
    "\n",
    "    error_message = 'Unsupported word2vec language'\n",
    "    raise Exception(error_message)\n",
    "\n",
    "def get_pretrained_matrix(language: Language):\n",
    "    data_path = os.path.join('..', 'data', 'ocr-evaluation', 'word2vec', language.value)\n",
    "    word2vec_model_name, word2vec_binary = get_word2vec_model_info(language)\n",
    "    word2vec_model_path = os.path.join(data_path, word2vec_model_name)\n",
    "    word2vec_model  = gensim.models.KeyedVectors.load_word2vec_format(word2vec_model_path, binary=word2vec_binary)\n",
    "    return word2vec_model, word2vec_model_path, word2vec_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "\n",
    "def create_model(\n",
    "    corpus,\n",
    "    model_path: str,\n",
    "    configuration: Configuration,\n",
    "    randomly_initialized: bool,\n",
    "    language: Language,\n",
    "    learning_rate: float):\n",
    "    sg = 1 if configuration == Configuration.SkipGram else 0\n",
    "    vector_size = 320 if language == Language.Dutch else 300\n",
    "\n",
    "    # initialize the model\n",
    "    model = Word2Vec(vector_size=vector_size, window=5, min_count=5, workers=2, sg=sg, alpha=learning_rate)\n",
    "\n",
    "    # build the vocabulary\n",
    "    model.build_vocab(corpus, progress_per=1000)\n",
    "\n",
    "    if not randomly_initialized:\n",
    "        word2vec_weights, word2vec_model_path, word2vec_binary = get_pretrained_matrix(language)\n",
    "        model.build_vocab(list(word2vec_weights.key_to_index.keys()), update=True)\n",
    "        model.wv.vectors_lockf = np.ones((len(model.wv.key_to_index), 1)) # fix for word2vec issue\n",
    "        model.wv.intersect_word2vec_format(word2vec_model_path, binary=word2vec_binary, lockf=1.0)\n",
    "\n",
    "    # train the model\n",
    "    model.train(corpus, total_examples=model.corpus_count, epochs=300, report_delay=1)\n",
    "\n",
    "    # save the model\n",
    "    model.save(model_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: ['french', cbow, lr: 0.001, True, ground-truth]\n",
      "Model is not loaded. Creating and training now...\n",
      "Training: ['french', cbow, lr: 0.001, True, raw]\n",
      "Model is not loaded. Creating and training now...\n",
      "Training: ['french', cbow, lr: 0.0001, True, ground-truth]\n",
      "Model is not loaded. Creating and training now...\n",
      "Training: ['french', cbow, lr: 0.0001, True, raw]\n",
      "Model is not loaded. Creating and training now...\n",
      "Training: ['french', skip-gram, lr: 0.001, True, ground-truth]\n",
      "Model is not loaded. Creating and training now...\n",
      "Training: ['french', skip-gram, lr: 0.001, True, raw]\n",
      "Model is not loaded. Creating and training now...\n",
      "Training: ['french', skip-gram, lr: 0.0001, True, ground-truth]\n",
      "Model is not loaded. Creating and training now...\n",
      "Training: ['french', skip-gram, lr: 0.0001, True, raw]\n",
      "Model is not loaded. Creating and training now...\n"
     ]
    }
   ],
   "source": [
    "unique_tokens = {}\n",
    "models = {}\n",
    "\n",
    "for language in [\n",
    "    # Language.Dutch,\n",
    "    # Language.English,\n",
    "    Language.French, \n",
    "    # Language.German\n",
    "    ]:\n",
    "    models[language] = {}\n",
    "    unique_tokens[language] = None\n",
    "    for configuration in [Configuration.CBOW, Configuration.SkipGram]:\n",
    "        models[language][configuration] = {}\n",
    "        for learning_rate in [0.001, 0.0001]:\n",
    "            models[language][configuration][learning_rate] = {}\n",
    "            for randomly_initialized in [True]:\n",
    "                models[language][configuration][learning_rate][randomly_initialized] = {}\n",
    "                for ocr_output_type in [OCROutputType.GroundTruth, OCROutputType.Raw]:\n",
    "                    print(f'Training: [\\'{language.value}\\', {configuration.value}, lr: {learning_rate}, {randomly_initialized}, {ocr_output_type.value}]')\n",
    "                    documents = read_documents(tokenizer, language, ocr_output_type)\n",
    "                    model_path = get_model_path(language, configuration, randomly_initialized, ocr_output_type, learning_rate)\n",
    "                    model = load_model(model_path)\n",
    "                    if model is None:\n",
    "                        print('Model is not loaded. Creating and training now...')\n",
    "                        model = create_model(documents, model_path, configuration, randomly_initialized, language, learning_rate)\n",
    "\n",
    "                    models[language][configuration][learning_rate][randomly_initialized][ocr_output_type] = model\n",
    "                    tokens = list(model.wv.key_to_index.keys())\n",
    "                    if unique_tokens[language] is None:\n",
    "                        unique_tokens[language] = tokens\n",
    "                    else:\n",
    "                        unique_tokens[language] = list(set(tokens) & set(unique_tokens[language]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_words = {\n",
    "    Language.English: ['man', 'new', 'time', 'day', 'good', 'old', 'little', 'one', 'two', 'three'],\n",
    "    Language.Dutch: ['man', 'jaar', 'tijd', 'dag', 'huis', 'dier', 'werk', 'naam', 'groot', 'kleine', 'twee', 'drie', 'vier', 'vijf']\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from gensim import similarities\n",
    "\n",
    "# similarities.MatrixSimilarity(vectors)\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# for word in target_words[language]:\n",
    "# #     print(f'-- \\'{word}\\':')\n",
    "# #     print(model.wv.most_similar(positive=[word]))\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_python_obj(obj: object, path: str, name: str) -> bool:\n",
    "    try:\n",
    "        filepath = os.path.join(path, f'{name}.pickle')\n",
    "        with open(filepath, 'wb') as handle:\n",
    "            pickle.dump(obj, handle, protocol=-1)\n",
    "\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def load_python_obj(path: str, name: str, extension_included: bool = False) -> object:\n",
    "    obj = None\n",
    "    try:\n",
    "        extension = '' if extension_included else '.pickle'\n",
    "        filepath = os.path.join(path, f'{name}{extension}')\n",
    "        with (open(filepath, \"rb\")) as openfile:\n",
    "            obj = pickle.load(openfile)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in [\n",
    "    # Language.Dutch, \n",
    "    # Language.English,\n",
    "    Language.French, \n",
    "    # Language.German\n",
    "    ]:\n",
    "    for config in [Configuration.SkipGram, Configuration.CBOW, Configuration.PPMI]:\n",
    "        cache_path = os.path.join('..', '.cache', 'ocr-evaluation', language.value, config.value)\n",
    "        raw_vocab_obj = load_python_obj(cache_path, f'vocab-icdar-2017-icdar-2019-{OCROutputType.Raw.value}')\n",
    "        gt_vocab_obj = load_python_obj(cache_path, f'vocab-icdar-2017-icdar-2019-{OCROutputType.GroundTruth.value}')\n",
    "        if raw_vocab_obj is None or gt_vocab_obj is None:\n",
    "            print(cache_path)\n",
    "            continue\n",
    "\n",
    "        # extract the tokens from the vocabularies\n",
    "        raw_tokens = list(raw_vocab_obj[0].keys())[4:]\n",
    "        gt_tokens = list(gt_vocab_obj[0].keys())[4:]\n",
    "        intersected_tokens = list(set(raw_tokens) & set(gt_tokens))\n",
    "        unique_tokens[language] = list(set(intersected_tokens) & set(unique_tokens[language]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = { language: {\n",
    "    word: idx for (idx, word) in enumerate(unique_tokens[language])\n",
    "}\n",
    "    for language in unique_tokens.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tokens for 'french, cbow, lr: 0.001, random': 100%|██████████| 18391/18391 [43:34<00:00,  7.03it/s]\n",
      "Processing tokens for 'french, cbow, lr: 0.0001, random': 100%|██████████| 18391/18391 [46:52<00:00,  6.54it/s]\n",
      "Processing tokens for 'french, skip-gram, lr: 0.001, random': 100%|██████████| 18391/18391 [45:30<00:00,  6.74it/s]\n",
      "Processing tokens for 'french, skip-gram, lr: 0.0001, random': 100%|██████████| 18391/18391 [46:49<00:00,  6.54it/s]\n"
     ]
    }
   ],
   "source": [
    "overlaps = {}\n",
    "percentages = list(range(1, 101, 1))  # 1..20\n",
    "\n",
    "for language in [\n",
    "    # Language.Dutch,\n",
    "    # Language.English,\n",
    "    Language.French, \n",
    "    # Language.German\n",
    "    ]:\n",
    "    overlaps[language] = {}\n",
    "    words_amounts = [ \n",
    "        int(len(unique_tokens[language]) * (float(percentage)/ 100))\n",
    "        for percentage in percentages]\n",
    "\n",
    "    max_n = max(words_amounts)\n",
    "\n",
    "    for configuration in [\n",
    "        Configuration.CBOW, \n",
    "        Configuration.SkipGram]:\n",
    "        overlaps[language][configuration] = {}\n",
    "        for learning_rate in [0.001, 0.0001]:\n",
    "            overlaps[language][configuration][learning_rate] = {}\n",
    "            for randomly_initialized in [True]:\n",
    "                rnd_suffix = 'random' if randomly_initialized else 'pretr'\n",
    "                cached_name = f'overlaps_{language.value}_{configuration.value}_lr{learning_rate}_{rnd_suffix}'\n",
    "                cached_value = load_python_obj('results', cached_name)\n",
    "                if cached_value is not None:\n",
    "                    overlaps[language][configuration][learning_rate][randomly_initialized] = cached_value\n",
    "                    continue\n",
    "\n",
    "                overlaps[language][configuration][learning_rate][randomly_initialized] = { percentage : { token: [] for token in unique_tokens[language] } for percentage in percentages }\n",
    "\n",
    "                raw_vectors = np.array([models[language][configuration][learning_rate][True][OCROutputType.Raw].wv[word] for word in unique_tokens[language]])\n",
    "                raw_similarity = 1 - cdist(raw_vectors, raw_vectors, metric='cosine')\n",
    "                gt_vectors = np.array([models[language][configuration][learning_rate][True][OCROutputType.GroundTruth].wv[word] for word in unique_tokens[language]])\n",
    "                gt_similarity = 1 - cdist(gt_vectors, gt_vectors, metric='cosine')\n",
    "\n",
    "                for token in tqdm(unique_tokens[language], desc=f'Processing tokens for \\'{language.value}, {configuration.value}, lr: {learning_rate}, {rnd_suffix}\\'', total=len(unique_tokens[language])):\n",
    "                    raw_indices = np.argsort(raw_similarity[_[language][token]])[::-1][:max_n]\n",
    "                    gt_indices = np.argsort(gt_similarity[_[language][token]])[::-1][:max_n]\n",
    "\n",
    "                    # gt_most_similar = models[language][configuration][learning_rate][randomly_initialized][OCROutputType.GroundTruth].wv.most_similar(token, topn=max_n)\n",
    "                    # gt_most_similar = [x[0] for x in gt_most_similar]\n",
    "                    # raw_most_similar = models[language][configuration][learning_rate][randomly_initialized][OCROutputType.Raw].wv.most_similar(token, topn=max_n)\n",
    "                    # raw_most_similar = [x[0] for x in raw_most_similar]\n",
    "\n",
    "                    for n, percentage in zip(words_amounts, percentages):\n",
    "                        # current_gt = gt_most_similar[:n]\n",
    "                        # current_raw = raw_most_similar[:n]\n",
    "                        current_gt = gt_indices[:n]\n",
    "                        current_raw = raw_indices[:n]\n",
    "\n",
    "                        current_overlaps = len(set(current_gt) & set(current_raw))\n",
    "                        overlaps[language][configuration][learning_rate][randomly_initialized][percentage][token].append(current_overlaps)\n",
    "\n",
    "                save_python_obj(overlaps[language][configuration][learning_rate][randomly_initialized], 'results', cached_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlaps[Language.Dutch][Configuration.CBOW][0.001][True][10]\n",
    "# overlaps[Language.Dutch][Configuration.CBOW][0.0001][True][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n",
      "320\n",
      "300\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(models[Language.Dutch][Configuration.SkipGram][0.001][True][OCROutputType.GroundTruth].layer1_size)\n",
    "print(models[Language.Dutch][Configuration.CBOW][0.001][True][OCROutputType.GroundTruth].layer1_size)\n",
    "print(models[Language.English][Configuration.SkipGram][0.001][True][OCROutputType.GroundTruth].layer1_size)\n",
    "print(models[Language.English][Configuration.CBOW][0.001][True][OCROutputType.GroundTruth].layer1_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.178571428571429\n",
      "8.92284718765555\n"
     ]
    }
   ],
   "source": [
    "# overlaps[Language.German][Configuration.CBOW][0.0001][True][1]\n",
    "print(np.mean(list(overlaps[Language.German][Configuration.CBOW][0.001][True][1].values())))\n",
    "print(np.mean(list(overlaps[Language.German][Configuration.CBOW][0.0001][True][1].values())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8b75f3199d40dc5b32c50d7e73d50b0653ef4e42fc29a92d9ddb9ff9d4b03964"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ocr': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
