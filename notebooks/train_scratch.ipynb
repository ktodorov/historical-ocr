{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\r\n",
    "from gensim.models import Word2Vec\r\n",
    "\r\n",
    "import os\r\n",
    "import re\r\n",
    "import string\r\n",
    "import numpy as np\r\n",
    "from tqdm import tqdm\r\n",
    "import _pickle as pickle\r\n",
    "\r\n",
    "from nltk.tokenize import RegexpTokenizer\r\n",
    "\r\n",
    "import sys\r\n",
    "sys.path.insert(0, '..')\r\n",
    "\r\n",
    "from enums.language import Language\r\n",
    "from enums.configuration import Configuration\r\n",
    "from enums.ocr_output_type import OCROutputType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\r\n",
    "\r\n",
    "def get_folder_paths(language: Language):\r\n",
    "    newseye_path = os.path.join('..', 'data', 'newseye')\r\n",
    "    icdar_2017_path = os.path.join(newseye_path, '2017', 'full')\r\n",
    "    icdar_2019_path = os.path.join(newseye_path, '2019', 'full')\r\n",
    "\r\n",
    "    result = None\r\n",
    "    if language == Language.English:\r\n",
    "        result = [\r\n",
    "            os.path.join(icdar_2017_path, 'eng_monograph'),\r\n",
    "            os.path.join(icdar_2017_path, 'eng_periodical'),\r\n",
    "            os.path.join(icdar_2019_path, 'EN')\r\n",
    "        ]\r\n",
    "    elif language == Language.Dutch:\r\n",
    "        result = [\r\n",
    "            os.path.join(icdar_2019_path, 'NL', 'NL1')\r\n",
    "        ]\r\n",
    "    elif language == Language.French:\r\n",
    "        result = [\r\n",
    "            os.path.join(icdar_2017_path, 'fr_monograph'),\r\n",
    "            os.path.join(icdar_2017_path, 'fr_periodical'),\r\n",
    "            os.path.join(icdar_2019_path, 'FR', 'FR1'),\r\n",
    "            os.path.join(icdar_2019_path, 'FR', 'FR2'),\r\n",
    "            os.path.join(icdar_2019_path, 'FR', 'FR3')\r\n",
    "        ]\r\n",
    "    elif language == Language.German:\r\n",
    "        result = [\r\n",
    "            os.path.join(icdar_2019_path, 'DE', 'DE1'),\r\n",
    "            os.path.join(icdar_2019_path, 'DE', 'DE2'),\r\n",
    "            os.path.join(icdar_2019_path, 'DE', 'DE3'),\r\n",
    "            os.path.join(icdar_2019_path, 'DE', 'DE4'),\r\n",
    "            os.path.join(icdar_2019_path, 'DE', 'DE5'),\r\n",
    "            os.path.join(icdar_2019_path, 'DE', 'DE6'),\r\n",
    "            os.path.join(icdar_2019_path, 'DE', 'DE7')\r\n",
    "        ]\r\n",
    "\r\n",
    "    return result\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents(tokenizer, language: Language, ocr_output_type: OCROutputType):\r\n",
    "    documents = []\r\n",
    "\r\n",
    "    folder_paths = get_folder_paths(language)\r\n",
    "    for folder_path in folder_paths:\r\n",
    "        for filename in os.listdir(folder_path):\r\n",
    "            file_path = os.path.join(folder_path, filename)\r\n",
    "            with open(file_path, 'r', encoding='utf-8') as text_file:\r\n",
    "                file_lines = text_file.readlines()\r\n",
    "                gt_line = file_lines[2] if ocr_output_type == OCROutputType.GroundTruth else file_lines[1]\r\n",
    "                processed_line = gt_line[14:].replace('#', '').replace('@', '')\r\n",
    "\r\n",
    "                text_nonum = re.sub(r'\\d+', '', processed_line)\r\n",
    "                text_nopunct = \"\".join([char.lower() for char in text_nonum if char not in string.punctuation])\r\n",
    "                text_no_doublespace = re.sub('\\s+', ' ', text_nopunct).strip()\r\n",
    "                result = tokenizer.tokenize(text_no_doublespace)\r\n",
    "                documents.append(result)\r\n",
    "\r\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_path(\r\n",
    "    language: Language,\r\n",
    "    configuration: Configuration,\r\n",
    "    randomly_initialized: bool,\r\n",
    "    ocr_output_type: OCROutputType,\r\n",
    "    learning_rate: float):\r\n",
    "    rnd_suffix = 'random' if randomly_initialized else 'pretr'\r\n",
    "\r\n",
    "    model_name = f'gensim_{language.value}_{configuration.value}_{rnd_suffix}_{ocr_output_type.value}_lr{learning_rate}.model'\r\n",
    "\r\n",
    "    results_folder = 'results'\r\n",
    "    if not os.path.exists(results_folder):\r\n",
    "        os.mkdir(results_folder)\r\n",
    "\r\n",
    "    result = os.path.join(results_folder, model_name)\r\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\r\n",
    "    if not os.path.exists(model_path):\r\n",
    "        return None\r\n",
    "\r\n",
    "    model = Word2Vec.load(model_path)\r\n",
    "    return model\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_model_info(language: Language):\r\n",
    "    if language == Language.English:\r\n",
    "        return 'GoogleNews-vectors-negative300.bin', True\r\n",
    "    elif language == Language.Dutch:\r\n",
    "        return 'combined-320.txt', False\r\n",
    "    elif language == Language.French:\r\n",
    "        return 'frwiki_20180420_300d.txt', False\r\n",
    "    elif language == Language.German:\r\n",
    "        return 'dewiki_20180420_300d.txt', False\r\n",
    "\r\n",
    "    error_message = 'Unsupported word2vec language'\r\n",
    "    raise Exception(error_message)\r\n",
    "\r\n",
    "def get_pretrained_matrix(language: Language):\r\n",
    "    data_path = os.path.join('..', 'data', 'ocr-evaluation', 'word2vec', language.value)\r\n",
    "    word2vec_model_name, word2vec_binary = get_word2vec_model_info(language)\r\n",
    "    word2vec_model_path = os.path.join(data_path, word2vec_model_name)\r\n",
    "    word2vec_model  = gensim.models.KeyedVectors.load_word2vec_format(word2vec_model_path, binary=word2vec_binary)\r\n",
    "    return word2vec_model, word2vec_model_path, word2vec_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\r\n",
    "\r\n",
    "def create_model(\r\n",
    "    corpus,\r\n",
    "    model_path: str,\r\n",
    "    configuration: Configuration,\r\n",
    "    randomly_initialized: bool,\r\n",
    "    language: Language,\r\n",
    "    learning_rate: float):\r\n",
    "    sg = 1 if configuration == Configuration.SkipGram else 0\r\n",
    "    vector_size = 320 if language == Language.Dutch else 300\r\n",
    "\r\n",
    "    # initialize the model\r\n",
    "    model = Word2Vec(vector_size=vector_size, window=5, min_count=5, workers=2, sg=sg, alpha=learning_rate)\r\n",
    "\r\n",
    "    # build the vocabulary\r\n",
    "    model.build_vocab(corpus, progress_per=1000)\r\n",
    "\r\n",
    "    if not randomly_initialized:\r\n",
    "        word2vec_weights, word2vec_model_path, word2vec_binary = get_pretrained_matrix(language)\r\n",
    "        model.build_vocab(list(word2vec_weights.key_to_index.keys()), update=True)\r\n",
    "        model.wv.vectors_lockf = np.ones((len(model.wv.key_to_index), 1)) # fix for word2vec issue\r\n",
    "        model.wv.intersect_word2vec_format(word2vec_model_path, binary=word2vec_binary, lockf=1.0)\r\n",
    "\r\n",
    "    # train the model\r\n",
    "    model.train(corpus, total_examples=model.corpus_count, epochs=300, report_delay=1)\r\n",
    "\r\n",
    "    # save the model\r\n",
    "    model.save(model_path)\r\n",
    "\r\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: ['dutch', cbow, lr: 0.001, True, ground-truth]\n",
      "Training: ['dutch', cbow, lr: 0.001, True, raw]\n",
      "Training: ['dutch', cbow, lr: 0.0001, True, ground-truth]\n",
      "Training: ['dutch', cbow, lr: 0.0001, True, raw]\n",
      "Training: ['dutch', skip-gram, lr: 0.001, True, ground-truth]\n",
      "Training: ['dutch', skip-gram, lr: 0.001, True, raw]\n",
      "Training: ['dutch', skip-gram, lr: 0.0001, True, ground-truth]\n",
      "Training: ['dutch', skip-gram, lr: 0.0001, True, raw]\n",
      "Training: ['english', cbow, lr: 0.001, True, ground-truth]\n",
      "Training: ['english', cbow, lr: 0.001, True, raw]\n",
      "Training: ['english', cbow, lr: 0.0001, True, ground-truth]\n",
      "Training: ['english', cbow, lr: 0.0001, True, raw]\n",
      "Training: ['english', skip-gram, lr: 0.001, True, ground-truth]\n",
      "Training: ['english', skip-gram, lr: 0.001, True, raw]\n",
      "Training: ['english', skip-gram, lr: 0.0001, True, ground-truth]\n",
      "Training: ['english', skip-gram, lr: 0.0001, True, raw]\n",
      "Training: ['french', cbow, lr: 0.001, True, ground-truth]\n",
      "Model is not loaded. Creating and training now...\n",
      "Training: ['french', cbow, lr: 0.001, True, raw]\n",
      "Model is not loaded. Creating and training now...\n",
      "Training: ['french', cbow, lr: 0.0001, True, ground-truth]\n",
      "Model is not loaded. Creating and training now...\n",
      "Training: ['french', cbow, lr: 0.0001, True, raw]\n",
      "Model is not loaded. Creating and training now...\n",
      "Training: ['french', skip-gram, lr: 0.001, True, ground-truth]\n",
      "Model is not loaded. Creating and training now...\n",
      "Training: ['french', skip-gram, lr: 0.001, True, raw]\n",
      "Model is not loaded. Creating and training now...\n",
      "Training: ['french', skip-gram, lr: 0.0001, True, ground-truth]\n",
      "Model is not loaded. Creating and training now...\n",
      "Training: ['french', skip-gram, lr: 0.0001, True, raw]\n",
      "Model is not loaded. Creating and training now...\n",
      "Training: ['german', cbow, lr: 0.001, True, ground-truth]\n",
      "Model is not loaded. Creating and training now...\n",
      "Training: ['german', cbow, lr: 0.001, True, raw]\n",
      "Model is not loaded. Creating and training now...\n",
      "Training: ['german', cbow, lr: 0.0001, True, ground-truth]\n",
      "Model is not loaded. Creating and training now...\n",
      "Training: ['german', cbow, lr: 0.0001, True, raw]\n",
      "Model is not loaded. Creating and training now...\n",
      "Training: ['german', skip-gram, lr: 0.001, True, ground-truth]\n",
      "Model is not loaded. Creating and training now...\n",
      "Training: ['german', skip-gram, lr: 0.001, True, raw]\n",
      "Model is not loaded. Creating and training now...\n",
      "Training: ['german', skip-gram, lr: 0.0001, True, ground-truth]\n",
      "Model is not loaded. Creating and training now...\n",
      "Training: ['german', skip-gram, lr: 0.0001, True, raw]\n",
      "Model is not loaded. Creating and training now...\n"
     ]
    }
   ],
   "source": [
    "unique_tokens = {}\r\n",
    "models = {}\r\n",
    "\r\n",
    "for language in [Language.Dutch, Language.English, Language.French, Language.German]:\r\n",
    "    models[language] = {}\r\n",
    "    unique_tokens[language] = None\r\n",
    "    for configuration in [Configuration.CBOW, Configuration.SkipGram]:\r\n",
    "        models[language][configuration] = {}\r\n",
    "        for learning_rate in [0.001, 0.0001]:\r\n",
    "            models[language][configuration][learning_rate] = {}\r\n",
    "            for randomly_initialized in [True]:\r\n",
    "                models[language][configuration][learning_rate][randomly_initialized] = {}\r\n",
    "                for ocr_output_type in [OCROutputType.GroundTruth, OCROutputType.Raw]:\r\n",
    "                    print(f'Training: [\\'{language.value}\\', {configuration.value}, lr: {learning_rate}, {randomly_initialized}, {ocr_output_type.value}]')\r\n",
    "                    documents = read_documents(tokenizer, language, ocr_output_type)\r\n",
    "                    model_path = get_model_path(language, configuration, randomly_initialized, ocr_output_type, learning_rate)\r\n",
    "                    model = load_model(model_path)\r\n",
    "                    if model is None:\r\n",
    "                        print('Model is not loaded. Creating and training now...')\r\n",
    "                        model = create_model(documents, model_path, configuration, randomly_initialized, language, learning_rate)\r\n",
    "\r\n",
    "                    models[language][configuration][learning_rate][randomly_initialized][ocr_output_type] = model\r\n",
    "                    tokens = list(model.wv.key_to_index.keys())\r\n",
    "                    if unique_tokens[language] is None:\r\n",
    "                        unique_tokens[language] = tokens\r\n",
    "                    else:\r\n",
    "                        unique_tokens[language] = list(set(tokens) & set(unique_tokens[language]))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_words = {\r\n",
    "    Language.English: ['man', 'new', 'time', 'day', 'good', 'old', 'little', 'one', 'two', 'three'],\r\n",
    "    Language.Dutch: ['man', 'jaar', 'tijd', 'dag', 'huis', 'dier', 'werk', 'naam', 'groot', 'kleine', 'twee', 'drie', 'vier', 'vijf']\r\n",
    "}\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# from gensim import similarities\r\n",
    "\r\n",
    "# similarities.MatrixSimilarity(vectors)\r\n",
    "\r\n",
    "from scipy.spatial.distance import cdist\r\n",
    "\r\n",
    "# for word in target_words[language]:\r\n",
    "# #     print(f'-- \\'{word}\\':')\r\n",
    "# #     print(model.wv.most_similar(positive=[word]))\r\n",
    "    \r\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_python_obj(obj: object, path: str, name: str) -> bool:\r\n",
    "    try:\r\n",
    "        filepath = os.path.join(path, f'{name}.pickle')\r\n",
    "        with open(filepath, 'wb') as handle:\r\n",
    "            pickle.dump(obj, handle, protocol=-1)\r\n",
    "\r\n",
    "        return True\r\n",
    "    except Exception:\r\n",
    "        return False\r\n",
    "\r\n",
    "def load_python_obj(path: str, name: str, extension_included: bool = False) -> object:\r\n",
    "    obj = None\r\n",
    "    try:\r\n",
    "        extension = '' if extension_included else '.pickle'\r\n",
    "        filepath = os.path.join(path, f'{name}{extension}')\r\n",
    "        with (open(filepath, \"rb\")) as openfile:\r\n",
    "            obj = pickle.load(openfile)\r\n",
    "\r\n",
    "    except FileNotFoundError:\r\n",
    "        return None\r\n",
    "\r\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\.cache\\ocr-evaluation\\french\\ppmi\n",
      "..\\.cache\\ocr-evaluation\\german\\ppmi\n"
     ]
    }
   ],
   "source": [
    "for language in [Language.Dutch, Language.English, Language.French, Language.German]:\r\n",
    "    for config in [Configuration.SkipGram, Configuration.CBOW, Configuration.PPMI]:\r\n",
    "        cache_path = os.path.join('..', '.cache', 'ocr-evaluation', language.value, config.value)\r\n",
    "        raw_vocab_obj = load_python_obj(cache_path, f'vocab-icdar-2017-icdar-2019-{OCROutputType.Raw.value}')\r\n",
    "        gt_vocab_obj = load_python_obj(cache_path, f'vocab-icdar-2017-icdar-2019-{OCROutputType.GroundTruth.value}')\r\n",
    "        if raw_vocab_obj is None or gt_vocab_obj is None:\r\n",
    "            print(cache_path)\r\n",
    "            continue\r\n",
    "\r\n",
    "        # extract the tokens from the vocabularies\r\n",
    "        raw_tokens = list(raw_vocab_obj[0].keys())[4:]\r\n",
    "        gt_tokens = list(gt_vocab_obj[0].keys())[4:]\r\n",
    "        intersected_tokens = list(set(raw_tokens) & set(gt_tokens))\r\n",
    "        unique_tokens[language] = list(set(intersected_tokens) & set(unique_tokens[language]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = { language: {\r\n",
    "    word: idx for (idx, word) in enumerate(unique_tokens[language])\r\n",
    "}\r\n",
    "    for language in unique_tokens.keys()\r\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tokens for 'french, cbow, lr: 0.001, random': 100%|██████████| 18391/18391 [55:53<00:00,  5.48it/s]\n",
      "Processing tokens for 'french, cbow, lr: 0.0001, random': 100%|██████████| 18391/18391 [56:02<00:00,  5.47it/s]\n",
      "Processing tokens for 'french, skip-gram, lr: 0.001, random': 100%|██████████| 18391/18391 [56:16<00:00,  5.45it/s]\n",
      "Processing tokens for 'french, skip-gram, lr: 0.0001, random': 100%|██████████| 18391/18391 [59:11<00:00,  5.18it/s]\n",
      "Processing tokens for 'german, cbow, lr: 0.001, random': 100%|██████████| 8036/8036 [11:03<00:00, 12.12it/s]\n",
      "Processing tokens for 'german, cbow, lr: 0.0001, random': 100%|██████████| 8036/8036 [11:10<00:00, 11.98it/s]\n",
      "Processing tokens for 'german, skip-gram, lr: 0.001, random': 100%|██████████| 8036/8036 [11:29<00:00, 11.66it/s]\n",
      "Processing tokens for 'german, skip-gram, lr: 0.0001, random': 100%|██████████| 8036/8036 [11:20<00:00, 11.81it/s]\n"
     ]
    }
   ],
   "source": [
    "overlaps = {}\r\n",
    "percentages = list(range(1, 101, 1))  # 1..20\r\n",
    "\r\n",
    "for language in [Language.Dutch, Language.English, Language.French, Language.German]:\r\n",
    "    overlaps[language] = {}\r\n",
    "    words_amounts = [ \r\n",
    "        int(len(unique_tokens[language]) * (float(percentage)/ 100))\r\n",
    "        for percentage in percentages]\r\n",
    "\r\n",
    "    max_n = max(words_amounts)\r\n",
    "\r\n",
    "    for configuration in [Configuration.CBOW, Configuration.SkipGram]:\r\n",
    "        overlaps[language][configuration] = {}\r\n",
    "        for learning_rate in [0.001, 0.0001]:\r\n",
    "            overlaps[language][configuration][learning_rate] = {}\r\n",
    "            for randomly_initialized in [True]:\r\n",
    "                rnd_suffix = 'random' if randomly_initialized else 'pretr'\r\n",
    "                cached_name = f'overlaps_{language.value}_{configuration.value}_lr{learning_rate}_{rnd_suffix}'\r\n",
    "                cached_value = load_python_obj('results', cached_name)\r\n",
    "                if cached_value is not None:\r\n",
    "                    overlaps[language][configuration][learning_rate][randomly_initialized] = cached_value\r\n",
    "                    continue\r\n",
    "\r\n",
    "                overlaps[language][configuration][learning_rate][randomly_initialized] = { percentage : { token: [] for token in unique_tokens[language] } for percentage in percentages }\r\n",
    "\r\n",
    "                raw_vectors = np.array([models[language][configuration][learning_rate][True][OCROutputType.Raw].wv[word] for word in unique_tokens[language]])\r\n",
    "                raw_similarity = 1 - cdist(raw_vectors, raw_vectors, metric='cosine')\r\n",
    "                gt_vectors = np.array([models[language][configuration][learning_rate][True][OCROutputType.GroundTruth].wv[word] for word in unique_tokens[language]])\r\n",
    "                gt_similarity = 1 - cdist(gt_vectors, gt_vectors, metric='cosine')\r\n",
    "\r\n",
    "                for token in tqdm(unique_tokens[language], desc=f'Processing tokens for \\'{language.value}, {configuration.value}, lr: {learning_rate}, {rnd_suffix}\\'', total=len(unique_tokens[language])):\r\n",
    "                    raw_indices = np.argsort(raw_similarity[_[language][token]])[::-1][:max_n]\r\n",
    "                    gt_indices = np.argsort(gt_similarity[_[language][token]])[::-1][:max_n]\r\n",
    "\r\n",
    "                    # gt_most_similar = models[language][configuration][learning_rate][randomly_initialized][OCROutputType.GroundTruth].wv.most_similar(token, topn=max_n)\r\n",
    "                    # gt_most_similar = [x[0] for x in gt_most_similar]\r\n",
    "                    # raw_most_similar = models[language][configuration][learning_rate][randomly_initialized][OCROutputType.Raw].wv.most_similar(token, topn=max_n)\r\n",
    "                    # raw_most_similar = [x[0] for x in raw_most_similar]\r\n",
    "\r\n",
    "                    for n, percentage in zip(words_amounts, percentages):\r\n",
    "                        # current_gt = gt_most_similar[:n]\r\n",
    "                        # current_raw = raw_most_similar[:n]\r\n",
    "                        current_gt = gt_indices[:n]\r\n",
    "                        current_raw = raw_indices[:n]\r\n",
    "\r\n",
    "                        current_overlaps = len(set(current_gt) & set(current_raw))\r\n",
    "                        overlaps[language][configuration][learning_rate][randomly_initialized][percentage][token].append(current_overlaps)\r\n",
    "\r\n",
    "                save_python_obj(overlaps[language][configuration][learning_rate][randomly_initialized], 'results', cached_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlaps[Language.Dutch][Configuration.CBOW][0.001][True][10]\r\n",
    "# overlaps[Language.Dutch][Configuration.CBOW][0.0001][True][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n",
      "320\n",
      "300\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(models[Language.Dutch][Configuration.SkipGram][0.001][True][OCROutputType.GroundTruth].layer1_size)\r\n",
    "print(models[Language.Dutch][Configuration.CBOW][0.001][True][OCROutputType.GroundTruth].layer1_size)\r\n",
    "print(models[Language.English][Configuration.SkipGram][0.001][True][OCROutputType.GroundTruth].layer1_size)\r\n",
    "print(models[Language.English][Configuration.CBOW][0.001][True][OCROutputType.GroundTruth].layer1_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc29f658ddb1b0f0a648f4c47acf5938bc6d1ad3f68ae93354e191176a755a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('ocr-uva': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}