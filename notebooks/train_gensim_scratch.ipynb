{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\r\n",
    "\r\n",
    "import os\r\n",
    "import re\r\n",
    "import string\r\n",
    "\r\n",
    "from nltk.tokenize import RegexpTokenizer\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\r\n",
    "\r\n",
    "newseye_path = os.path.join('..', 'data', 'newseye')\r\n",
    "icdar_2017_1_path = os.path.join(newseye_path, '2017', 'full', 'eng_monograph')\r\n",
    "icdar_2017_2_path = os.path.join(newseye_path, '2017', 'full', 'eng_periodical')\r\n",
    "icdar_2019_path = os.path.join(newseye_path, '2019', 'full', 'EN')\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\r\n",
    "\r\n",
    "for icdar_path in [icdar_2017_1_path, icdar_2017_2_path, icdar_2019_path]:\r\n",
    "    for filename in os.listdir(icdar_path):\r\n",
    "        file_path = os.path.join(icdar_path, filename)\r\n",
    "        with open(file_path, 'r', encoding='utf-8') as text_file:\r\n",
    "            file_lines = text_file.readlines()\r\n",
    "            gt_line = file_lines[2]\r\n",
    "            processed_line = gt_line.replace('[ GS_aligned]', '').replace('#', '').replace('@', '')\r\n",
    "\r\n",
    "            text_nonum = re.sub(r'\\d+', '', processed_line)\r\n",
    "            text_nopunct = \"\".join([char.lower() for char in text_nonum if char not in string.punctuation])\r\n",
    "            text_no_doublespace = re.sub('\\s+', ' ', text_nopunct).strip()\r\n",
    "            result = tokenizer.tokenize(text_no_doublespace)\r\n",
    "            documents.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'gensim_default_eng.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\r\n",
    "    if not os.path.exists(model_path):\r\n",
    "        return None\r\n",
    "\r\n",
    "    model = Word2Vec.load(model_path)\r\n",
    "    return model\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\r\n",
    "\r\n",
    "def create_model(corpus):\r\n",
    "    model = Word2Vec(vector_size=300, window=5, min_count=5, workers=2)\r\n",
    "    model.build_vocab(corpus, progress_per=10000)\r\n",
    "    model.train(corpus, total_examples=model.corpus_count, epochs=300, report_delay=1)\r\n",
    "    model.save()\r\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model()\r\n",
    "if model is None:\r\n",
    "    print('Model is not loaded. Creating and training now...')\r\n",
    "    model = create_model(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- 'man':\n",
      "[('he', 0.430123507976532), ('i', 0.37541136145591736), ('it', 0.36927860975265503), ('woman', 0.3595450818538666), ('gentleman', 0.35697460174560547), ('him', 0.35161837935447693), ('mans', 0.3401086628437042), ('and', 0.32658615708351135), ('that', 0.3250766396522522), ('so', 0.3246637284755707)]\n",
      "-- 'new':\n",
      "[('annuity', 0.21009431779384613), ('unpublished', 0.2047516405582428), ('teaching', 0.19760270416736603), ('middlesbrough', 0.1955123245716095), ('supplied', 0.1879047453403473), ('quince', 0.18704593181610107), ('british', 0.18611449003219604), ('cards', 0.18508203327655792), ('homer', 0.17994344234466553), ('appointed', 0.17965558171272278)]\n",
      "-- 'time':\n",
      "[('he', 0.36121225357055664), ('it', 0.3312700092792511), ('that', 0.2923469841480255), ('them', 0.29134300351142883), ('period', 0.2901051938533783), ('and', 0.28276896476745605), ('but', 0.2818032205104828), ('i', 0.27945730090141296), ('was', 0.2750898003578186), ('the', 0.2749282717704773)]\n",
      "-- 'day':\n",
      "[('morning', 0.31954115629196167), ('at', 0.2845919728279114), ('time', 0.26079443097114563), ('week', 0.259572297334671), ('monday', 0.24978791177272797), ('evening', 0.23731312155723572), ('hour', 0.2359606772661209), ('moment', 0.23210389912128448), ('and', 0.2287309467792511), ('night', 0.2263869196176529)]\n",
      "-- 'good':\n",
      "[('for', 0.40225639939308167), ('that', 0.3779967427253723), ('and', 0.3586083650588989), ('to', 0.3570370674133301), ('you', 0.34554755687713623), ('it', 0.3395287096500397), ('but', 0.336474746465683), ('i', 0.3334042429924011), ('not', 0.3332188129425049), ('said', 0.30754902958869934)]\n",
      "-- 'old':\n",
      "[('a', 0.2600916624069214), ('hush', 0.2365724742412567), ('elderly', 0.21494492888450623), ('elm', 0.2132076621055603), ('the', 0.20546966791152954), ('and', 0.20124299824237823), ('young', 0.1983904391527176), ('on', 0.19838328659534454), ('behind', 0.19831280410289764), ('up', 0.19219335913658142)]\n",
      "-- 'little':\n",
      "[('small', 0.208921879529953), ('going', 0.1936653107404709), ('no', 0.18756793439388275), ('a', 0.18674591183662415), ('gentle', 0.18448324501514435), ('ground', 0.18040181696414948), ('woman', 0.17898210883140564), ('much', 0.17818592488765717), ('sun', 0.17762963473796844), ('goodlooking', 0.17579606175422668)]\n",
      "-- 'one':\n",
      "[('the', 0.49902498722076416), ('a', 0.493473619222641), ('and', 0.45367538928985596), ('it', 0.4426438510417938), ('in', 0.4399796724319458), ('that', 0.4192136526107788), ('this', 0.417976051568985), ('for', 0.38942182064056396), ('of', 0.38819724321365356), ('to', 0.3868567645549774)]\n",
      "-- 'two':\n",
      "[('four', 0.4164268374443054), ('three', 0.3913613259792328), ('five', 0.3883998394012451), ('ﬁve', 0.3475306034088135), ('ten', 0.3174246549606323), ('several', 0.31577569246292114), ('six', 0.30777907371520996), ('eight', 0.30748820304870605), ('twelve', 0.304951936006546), ('twenty', 0.3048650622367859)]\n",
      "-- 'three':\n",
      "[('eight', 0.39494022727012634), ('two', 0.3913612961769104), ('four', 0.3878433406352997), ('six', 0.367939293384552), ('ten', 0.3329663872718811), ('nine', 0.30149251222610474), ('twelve', 0.30095335841178894), ('ﬁve', 0.2976764440536499), ('thirteen', 0.29659345746040344), ('five', 0.2958443760871887)]\n"
     ]
    }
   ],
   "source": [
    "words = ['man', 'new', 'time', 'day', 'good', 'old', 'little', 'one', 'two', 'three']\r\n",
    "for word in words:\r\n",
    "    print(f'-- \\'{word}\\':')\r\n",
    "    print(model.wv.most_similar(positive=[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('ocr-uva': conda)",
   "name": "python392jvsc74a57bd0cc29f658ddb1b0f0a648f4c47acf5938bc6d1ad3f68ae93354e191176a755a49"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}